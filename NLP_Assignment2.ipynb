{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThEvFAQCL2j9",
        "outputId": "14b4be31-c3e4-4240-bf7f-0784ea0f62b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Corpus:\n",
            "natural language processing fun exciting\n",
            "machine learning deep learning subsets ai\n",
            "field nlp involves computer science linguistics statistics\n",
            "word embeddings like word2vec capture semantic relationships words\n",
            "data preprocessing crucial step machine learning workflows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "corpus = [\n",
        "    \"Natural Language Processing is fun and exciting.\",\n",
        "    \"Machine learning and deep learning are subsets of AI.\",\n",
        "    \"The field of NLP involves computer science, linguistics, and statistics.\",\n",
        "    \"Word embeddings like Word2Vec capture semantic relationships between words.\",\n",
        "    \"Data preprocessing is a crucial step in machine learning workflows.\"\n",
        "]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[' + string.punctuation + ']', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return ' '.join(tokens), tokens\n",
        "\n",
        "\n",
        "processed_corpus = []\n",
        "tokenized_corpus = []\n",
        "\n",
        "for doc in corpus:\n",
        "    processed_text, tokens = preprocess_text(doc)\n",
        "    processed_corpus.append(processed_text)\n",
        "    tokenized_corpus.append(tokens)\n",
        "\n",
        "print(\"Processed Corpus:\")\n",
        "for doc in processed_corpus:\n",
        "    print(doc)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "bow_counts = count_vectorizer.fit_transform(processed_corpus)\n",
        "\n",
        "bow_df = pd.DataFrame(bow_counts.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
        "print(\"Bag-of-Words (Raw Counts):\")\n",
        "print(bow_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzgJcSyGOmhw",
        "outputId": "a7ba3f8c-c850-4b06-bedb-3479d0bac2cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words (Raw Counts):\n",
            "   ai  capture  computer  crucial  data  deep  embeddings  exciting  field  \\\n",
            "0   0        0         0        0     0     0           0         1      0   \n",
            "1   1        0         0        0     0     1           0         0      0   \n",
            "2   0        0         1        0     0     0           0         0      1   \n",
            "3   0        1         0        0     0     0           1         0      0   \n",
            "4   0        0         0        1     1     0           0         0      0   \n",
            "\n",
            "   fun  ...  relationships  science  semantic  statistics  step  subsets  \\\n",
            "0    1  ...              0        0         0           0     0        0   \n",
            "1    0  ...              0        0         0           0     0        1   \n",
            "2    0  ...              0        1         0           1     0        0   \n",
            "3    0  ...              1        0         1           0     0        0   \n",
            "4    0  ...              0        0         0           0     1        0   \n",
            "\n",
            "   word  word2vec  words  workflows  \n",
            "0     0         0      0          0  \n",
            "1     0         0      0          0  \n",
            "2     0         0      0          0  \n",
            "3     1         1      1          0  \n",
            "4     0         0      0          1  \n",
            "\n",
            "[5 rows x 30 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_counts(count_matrix):\n",
        "    counts = count_matrix.toarray().astype(float)\n",
        "    row_sums = counts.sum(axis=1, keepdims=True)\n",
        "    normalized = counts / (row_sums + 1e-10)\n",
        "    return normalized\n",
        "\n",
        "normalized_bow = normalize_counts(bow_counts)\n",
        "normalized_bow_df = pd.DataFrame(normalized_bow, columns=count_vectorizer.get_feature_names_out())\n",
        "print(\"\\nBag-of-Words (Normalized Counts):\")\n",
        "print(normalized_bow_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhVqLegZOpZm",
        "outputId": "9e0e4875-b39e-453d-c25e-8dde6e99ebf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bag-of-Words (Normalized Counts):\n",
            "         ai  capture  computer   crucial      data      deep  embeddings  \\\n",
            "0  0.000000    0.000  0.000000  0.000000  0.000000  0.000000       0.000   \n",
            "1  0.166667    0.000  0.000000  0.000000  0.000000  0.166667       0.000   \n",
            "2  0.000000    0.000  0.142857  0.000000  0.000000  0.000000       0.000   \n",
            "3  0.000000    0.125  0.000000  0.000000  0.000000  0.000000       0.125   \n",
            "4  0.000000    0.000  0.000000  0.142857  0.142857  0.000000       0.000   \n",
            "\n",
            "   exciting     field  fun  ...  relationships   science  semantic  \\\n",
            "0       0.2  0.000000  0.2  ...          0.000  0.000000     0.000   \n",
            "1       0.0  0.000000  0.0  ...          0.000  0.000000     0.000   \n",
            "2       0.0  0.142857  0.0  ...          0.000  0.142857     0.000   \n",
            "3       0.0  0.000000  0.0  ...          0.125  0.000000     0.125   \n",
            "4       0.0  0.000000  0.0  ...          0.000  0.000000     0.000   \n",
            "\n",
            "   statistics      step   subsets   word  word2vec  words  workflows  \n",
            "0    0.000000  0.000000  0.000000  0.000     0.000  0.000   0.000000  \n",
            "1    0.000000  0.000000  0.166667  0.000     0.000  0.000   0.000000  \n",
            "2    0.142857  0.000000  0.000000  0.000     0.000  0.000   0.000000  \n",
            "3    0.000000  0.000000  0.000000  0.125     0.125  0.125   0.000000  \n",
            "4    0.000000  0.142857  0.000000  0.000     0.000  0.000   0.142857  \n",
            "\n",
            "[5 rows x 30 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 100\n",
        "window_size = 5\n",
        "min_word_count = 1\n",
        "workers = 4\n",
        "\n",
        "w2v_model = Word2Vec(sentences=tokenized_corpus,\n",
        "                     vector_size=embedding_size,\n",
        "                     window=window_size,\n",
        "                     min_count=min_word_count,\n",
        "                     workers=workers,\n",
        "                     sg=1)\n",
        "\n",
        "word = 'nlp'\n",
        "if word in w2v_model.wv:\n",
        "    print(f\"\\nWord2Vec embedding for '{word}':\")\n",
        "    print(w2v_model.wv[word])\n",
        "else:\n",
        "    print(f\"\\nWord '{word}' not found in vocabulary.\")\n",
        "\n",
        "print(\"\\nWords most similar to 'nlp':\")\n",
        "if 'nlp' in w2v_model.wv:\n",
        "    similar_words = w2v_model.wv.most_similar('nlp', topn=5)\n",
        "    for word, similarity in similar_words:\n",
        "        print(f\"{word}: {similarity:.4f}\")\n",
        "else:\n",
        "    print(\"Word 'nlp' not found in vocabulary.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shpbjic3O0rm",
        "outputId": "846f9530-0f26-441f-a058-470f15520495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word2Vec embedding for 'nlp':\n",
            "[-1.9442164e-03 -5.2675214e-03  9.4471136e-03 -9.2987325e-03\n",
            "  4.5039477e-03  5.4041781e-03 -1.4092624e-03  9.0070926e-03\n",
            "  9.8853596e-03 -5.4750429e-03 -6.0210000e-03 -6.7469729e-03\n",
            " -7.8948820e-03 -3.0479168e-03 -5.5940272e-03 -8.3446801e-03\n",
            "  7.8290224e-04  2.9946566e-03  6.4147436e-03 -2.6289499e-03\n",
            " -4.4534765e-03  1.2495709e-03  3.9146186e-04  8.1169987e-03\n",
            "  1.8280029e-04  7.2315861e-03 -8.2645155e-03  8.4335366e-03\n",
            " -1.8889094e-03  8.7011540e-03 -7.6168370e-03  1.7963862e-03\n",
            "  1.0564864e-03  4.6005251e-05 -5.1032533e-03 -9.2476979e-03\n",
            " -7.2642174e-03 -7.9511739e-03  1.9137275e-03  4.7846674e-04\n",
            " -1.8131376e-03  7.1201660e-03 -2.4756920e-03 -1.3473093e-03\n",
            " -8.9005642e-03 -9.9254129e-03  8.9493981e-03 -5.7539381e-03\n",
            " -6.3729975e-03  5.1994072e-03  6.6699935e-03 -6.8316413e-03\n",
            "  9.5975993e-04 -6.0084737e-03  1.6473436e-03 -4.2892788e-03\n",
            " -3.4407973e-03  2.1856665e-03  8.6615775e-03  6.7281104e-03\n",
            " -9.6770572e-03 -5.6221043e-03  7.8803329e-03  1.9893574e-03\n",
            " -4.2560520e-03  5.9881213e-04  9.5209610e-03 -1.1027169e-03\n",
            " -9.4246380e-03  1.6084099e-03  6.2323548e-03  6.2823701e-03\n",
            "  4.0916502e-03 -5.6502391e-03 -3.7069322e-04 -5.5317880e-05\n",
            "  4.5717955e-03 -8.0415895e-03 -8.0183093e-03  2.6475071e-04\n",
            " -8.6082993e-03  5.8201565e-03 -4.1781188e-04  9.9711772e-03\n",
            " -5.3439774e-03 -4.8613906e-04  7.7567734e-03 -4.0679323e-03\n",
            " -5.0159004e-03  1.5900708e-03  2.6506938e-03 -2.5649595e-03\n",
            "  6.4475285e-03 -7.6599526e-03  3.3935606e-03  4.8997044e-04\n",
            "  8.7321829e-03  5.9827138e-03  6.8153618e-03  7.8225443e-03]\n",
            "\n",
            "Words most similar to 'nlp':\n",
            "language: 0.1502\n",
            "word2vec: 0.1490\n",
            "ai: 0.1281\n",
            "semantic: 0.1139\n",
            "preprocessing: 0.0972\n"
          ]
        }
      ]
    }
  ]
}